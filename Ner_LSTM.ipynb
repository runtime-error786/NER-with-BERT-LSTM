{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"eriktks/conll2003\", trust_remote_code=True)\n",
    "\n",
    "tags = dataset['train'].features['ner_tags'].feature\n",
    "tag2idx = tags.str2int\n",
    "idx2tag = tags.int2str\n",
    "\n",
    "word2idx = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for sentence in dataset[split][\"tokens\"]:\n",
    "        for word in sentence:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx) + 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "def encode_data(dataset_split, max_len, word2idx, tag2idx):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for tokens, ner_tags in zip(dataset_split[\"tokens\"], dataset_split[\"ner_tags\"]):\n",
    "        encoded_sentence = [word2idx.get(word, 0) for word in tokens]\n",
    "        encoded_tags = ner_tags\n",
    "\n",
    "        encoded_sentence = encoded_sentence[:max_len] + [0] * (max_len - len(encoded_sentence))\n",
    "        encoded_tags = encoded_tags[:max_len] + [tag2idx(\"O\")] * (max_len - len(encoded_tags))\n",
    "\n",
    "        sentences.append(encoded_sentence)\n",
    "        labels.append(encoded_tags)\n",
    "    return torch.tensor(sentences), torch.tensor(labels)\n",
    "\n",
    "train_sentences, train_tags = encode_data(dataset[\"train\"], MAX_LEN, word2idx, tag2idx)\n",
    "val_sentences, val_tags = encode_data(dataset[\"validation\"], MAX_LEN, word2idx, tag2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "train_dataset = NERDataset(train_sentences, train_tags)\n",
    "val_dataset = NERDataset(val_sentences, val_tags)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "        super(NERLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NERLSTM(VOCAB_SIZE, TAGSET_SIZE, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for sentences, tags in train_loader:\n",
    "            sentences, tags = sentences.to(\"cpu\"), tags.to(\"cpu\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sentences)\n",
    "\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, tags)\n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            total_loss += loss.item()  \n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, optimizer, criterion, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, val_loader, idx2tag):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in val_loader:\n",
    "            sentences, tags = sentences.to(\"cpu\"), tags.to(\"cpu\")\n",
    "            outputs = model(sentences)\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy().flatten())\n",
    "            all_tags.extend(tags.cpu().numpy().flatten())\n",
    "\n",
    "    valid_preds = [p for p, t in zip(all_preds, all_tags) if t != 0]\n",
    "    valid_tags = [t for t in all_tags if t != 0]\n",
    "\n",
    "    valid_preds = [idx2tag[p] for p in valid_preds]\n",
    "    valid_tags = [idx2tag[t] for t in valid_tags]\n",
    "\n",
    "    target_names = [tag for idx, tag in idx2tag.items() if idx != 0]\n",
    "\n",
    "    print(classification_report(valid_tags, valid_preds, target_names=target_names))\n",
    "\n",
    "evaluate_model(model, val_loader, idx2tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sentence, word2idx, idx2tag, max_len):\n",
    "    model.eval()\n",
    "    sentence_idx = [word2idx.get(word, 0) for word in sentence]\n",
    "    padded_sentence = sentence_idx[:max_len] + [0] * (max_len - len(sentence_idx))\n",
    "    input_tensor = torch.tensor([padded_sentence], dtype=torch.long).to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        predictions = torch.argmax(outputs, dim=2).cpu().numpy()[0]\n",
    "\n",
    "    return [(word, idx2tag[pred]) for word, pred in zip(sentence, predictions[:len(sentence)])]\n",
    "\n",
    "test_sentence = [\"Mustafa\", \"Rizwan\", \"lives\", \"in\", \"Pakistan\"]\n",
    "print(predict(model, test_sentence, word2idx, idx2tag, MAX_LEN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
